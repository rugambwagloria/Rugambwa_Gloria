{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "936cb101",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5074542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('Churn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12904a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.to_string of       Gender  Senior Citizen Partner Dependents  tenure Phone Service  \\\n",
       "0     Female               0     Yes         No       1            No   \n",
       "1     Female               0     Yes         No       1            No   \n",
       "2       Male               0      No         No      34           Yes   \n",
       "3       Male               0      No         No       2           Yes   \n",
       "4       Male               0      No         No      45            No   \n",
       "...      ...             ...     ...        ...     ...           ...   \n",
       "7039    Male               0     Yes        Yes      24           Yes   \n",
       "7040  Female               0     Yes        Yes      72           Yes   \n",
       "7041  Female               0     Yes        Yes      11            No   \n",
       "7042    Male               1     Yes         No       4           Yes   \n",
       "7043    Male               0      No         No      66           Yes   \n",
       "\n",
       "        Multiple Lines Internet Service Online Security Online Backup  \\\n",
       "0     No phone service              DSL              No           Yes   \n",
       "1     No phone service              DSL              No           Yes   \n",
       "2                   No              DSL             Yes            No   \n",
       "3                   No              DSL             Yes           Yes   \n",
       "4     No phone service              DSL             Yes            No   \n",
       "...                ...              ...             ...           ...   \n",
       "7039               Yes              DSL             Yes            No   \n",
       "7040               Yes      Fiber optic              No           Yes   \n",
       "7041  No phone service              DSL             Yes            No   \n",
       "7042               Yes      Fiber optic              No            No   \n",
       "7043                No      Fiber optic             Yes            No   \n",
       "\n",
       "     Device Protection Tech Support Streaming TV Streaming Movies  \\\n",
       "0                   No           No           No               No   \n",
       "1                   No           No           No               No   \n",
       "2                  Yes           No           No               No   \n",
       "3                   No           No           No               No   \n",
       "4                  Yes          Yes           No               No   \n",
       "...                ...          ...          ...              ...   \n",
       "7039               Yes          Yes          Yes              Yes   \n",
       "7040               Yes           No          Yes              Yes   \n",
       "7041                No           No           No               No   \n",
       "7042                No           No           No               No   \n",
       "7043               Yes          Yes          Yes              Yes   \n",
       "\n",
       "            Contract Paperless Billing             Payment Method  \\\n",
       "0     Month-to-month               Yes           Electronic check   \n",
       "1     Month-to-month               Yes           Electronic check   \n",
       "2           One year                No               Mailed check   \n",
       "3     Month-to-month               Yes               Mailed check   \n",
       "4           One year                No  Bank transfer (automatic)   \n",
       "...              ...               ...                        ...   \n",
       "7039        One year               Yes               Mailed check   \n",
       "7040        One year               Yes    Credit card (automatic)   \n",
       "7041  Month-to-month               Yes           Electronic check   \n",
       "7042  Month-to-month               Yes               Mailed check   \n",
       "7043        Two year               Yes  Bank transfer (automatic)   \n",
       "\n",
       "      Monthly Charges Total Charges Churn  \n",
       "0               29.85         29.85    No  \n",
       "1               29.85         29.85    No  \n",
       "2               56.95        1889.5    No  \n",
       "3               53.85        108.15   Yes  \n",
       "4               42.30       1840.75    No  \n",
       "...               ...           ...   ...  \n",
       "7039            84.80        1990.5    No  \n",
       "7040           103.20        7362.9    No  \n",
       "7041            29.60        346.45    No  \n",
       "7042            74.40         306.6   Yes  \n",
       "7043           105.65        6844.5    No  \n",
       "\n",
       "[7044 rows x 20 columns]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a6eee54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop customer ID as it's not useful for prediction\n",
    "df = df.drop('Customer ID', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "252747e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert target variable to binary (0/1)\n",
    "df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1816f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical variables to numerical\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "\tif df[col].nunique() == 2:\n",
    "\t\t# Example: map binary categorical columns to 0/1\n",
    "\t\tdf[col] = df[col].map({df[col].unique()[0]: 0, df[col].unique()[1]: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29db6c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop('Customer ID', axis=1)  # Already dropped in a previous cell\n",
    "\n",
    "# Convert target variable to binary (0/1)\n",
    "df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Convert categorical variables to numerical\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    if df[col].nunique() == 2:\n",
    "        # Binary encode columns with only 2 unique values\n",
    "        df[col] = LabelEncoder().fit_transform(df[col])\n",
    "    else:\n",
    "        # One-hot encode columns with more than 2 unique values\n",
    "        df = pd.get_dummies(df, columns=[col], prefix=col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ec960b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into features and target\n",
    "X = df.drop('Churn', axis=1)\n",
    "y = df['Churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ffb1bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid samples available for splitting. Please check your preprocessing steps and ensure the target variable contains valid values.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ensure there are valid (non-NaN) target values before splitting\n",
    "\n",
    "valid_indices = y[~y.isna()].index  # Get indices of non-NaN y values\n",
    "X_nonan = X.loc[valid_indices]      # Use loc for label-based indexing\n",
    "y_nonan = y.loc[valid_indices]\n",
    "\n",
    "if len(X_nonan) == 0 or len(y_nonan) == 0:\n",
    "    print(\"No valid samples available for splitting. Please check your preprocessing steps and ensure the target variable contains valid values.\")\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_nonan, \n",
    "        y_nonan, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        stratify=y_nonan  # Only works if y has at least 2 classes\n",
    "    )\n",
    "\n",
    "    # Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)  # Returns numpy array\n",
    "    X_test = scaler.transform(X_test)        # Must use same scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "388e58e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train is not defined or is empty. Please check your data preprocessing steps.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential, layers, metrics\n",
    "\n",
    "def create_model(input_dim):\n",
    "    return Sequential([\n",
    "        layers.Dense(64, activation='relu', input_dim=input_dim),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "# Check if X_train exists and is not empty\n",
    "if 'X_train' in locals() and X_train is not None and hasattr(X_train, 'shape'):\n",
    "    model = create_model(X_train.shape[1])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', metrics.Precision(), metrics.Recall()])\n",
    "    model.summary()\n",
    "else:\n",
    "    print(\"X_train is not defined or is empty. Please check your data preprocessing steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86a73752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model or training data is not defined or is empty. Please check your preprocessing steps and ensure you have valid data before training.\n"
     ]
    }
   ],
   "source": [
    "# Define early stopping to prevent overfitting\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Check if model, X_train, and y_train are defined and not empty\n",
    "if 'model' in locals() and model is not None and \\\n",
    "   'X_train' in locals() and X_train is not None and hasattr(X_train, 'shape') and X_train.shape[0] > 0 and \\\n",
    "   'y_train' in locals() and y_train is not None and hasattr(y_train, 'shape') and y_train.shape[0] > 0:\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "else:\n",
    "    print(\"Model or training data is not defined or is empty. Please check your preprocessing steps and ensure you have valid data before training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5878852c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model or test data is not defined or is empty. Please check your preprocessing steps and ensure you have valid data before evaluation.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set only if model and test data are available\n",
    "if 'model' in locals() and model is not None and \\\n",
    "   'X_test' in locals() and X_test is not None and hasattr(X_test, 'shape') and X_test.shape[0] > 0 and \\\n",
    "   'y_test' in locals() and y_test is not None and hasattr(y_test, 'shape') and y_test.shape[0] > 0:\n",
    "\ttest_loss, test_acc, test_precision, test_recall = model.evaluate(X_test, y_test, verbose=0)\n",
    "\tprint(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\tprint(f\"Test Precision: {test_precision:.4f}\")\n",
    "\tprint(f\"Test Recall: {test_recall:.4f}\")\n",
    "\n",
    "\t# Make predictions\n",
    "\ty_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "\n",
    "\t# Classification report\n",
    "\tprint(\"\\nClassification Report:\")\n",
    "\tprint(classification_report(y_test, y_pred))\n",
    "\n",
    "\t# Confusion matrix\n",
    "\tprint(\"\\nConfusion Matrix:\")\n",
    "\tprint(confusion_matrix(y_test, y_pred))\n",
    "else:\n",
    "\tprint(\"Model or test data is not defined or is empty. Please check your preprocessing steps and ensure you have valid data before evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c555b050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model or data is not defined or is empty. Please check your preprocessing steps and ensure you have valid data before training and evaluation.\n"
     ]
    }
   ],
   "source": [
    "# Check if model and data are defined and not empty before training and evaluation\n",
    "if 'model' in locals() and model is not None and \\\n",
    "   'X_train' in locals() and X_train is not None and hasattr(X_train, 'shape') and X_train.shape[0] > 0 and \\\n",
    "   'y_train' in locals() and y_train is not None and hasattr(y_train, 'shape') and y_train.shape[0] > 0 and \\\n",
    "   'X_test' in locals() and X_test is not None and hasattr(X_test, 'shape') and X_test.shape[0] > 0 and \\\n",
    "   'y_test' in locals() and y_test is not None and hasattr(y_test, 'shape') and y_test.shape[0] > 0:\n",
    "    # First ensure the model is trained\n",
    "    history = model.fit(X_train, y_train, epochs=10, validation_split=0.2)\n",
    "\n",
    "    # Then evaluate\n",
    "    print(\"\\nEvaluation:\")\n",
    "    print(model.metrics_names)  # Debug output\n",
    "    results = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "    if len(results) == 4:  # If you get 4 values as expected\n",
    "        test_loss, test_acc, test_precision, test_recall = results\n",
    "        print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "        print(f\"Test Precision: {test_precision:.4f}\")\n",
    "        print(f\"Test Recall: {test_recall:.4f}\")\n",
    "    else:\n",
    "        print(\"Unexpected number of metrics returned:\", results)\n",
    "\n",
    "    # Predictions (this part should work regardless)\n",
    "    y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "else:\n",
    "    print(\"Model or data is not defined or is empty. Please check your preprocessing steps and ensure you have valid data before training and evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6a8121a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation failed with the following errors:\n",
      "- Model not defined\n",
      "- X_train not defined\n",
      "- y_train not defined\n",
      "- X_test not defined\n",
      "- y_test not defined\n",
      "Please fix these issues before proceeding.\n"
     ]
    }
   ],
   "source": [
    "# List to collect error messages\n",
    "errors = []\n",
    "\n",
    "# Check each condition individually\n",
    "if 'model' not in locals():\n",
    "    errors.append(\"Model not defined\")\n",
    "elif model is None:\n",
    "    errors.append(\"Model is None\")\n",
    "\n",
    "if 'X_train' not in locals():\n",
    "    errors.append(\"X_train not defined\")\n",
    "elif X_train is None:\n",
    "    errors.append(\"X_train is None\")\n",
    "elif not hasattr(X_train, 'shape'):\n",
    "    errors.append(\"X_train has no shape attribute\")\n",
    "elif X_train.shape[0] == 0:\n",
    "    errors.append(\"X_train is empty (0 rows)\")\n",
    "\n",
    "if 'y_train' not in locals():\n",
    "    errors.append(\"y_train not defined\")\n",
    "elif y_train is None:\n",
    "    errors.append(\"y_train is None\")\n",
    "elif not hasattr(y_train, 'shape'):\n",
    "    errors.append(\"y_train has no shape attribute\")\n",
    "elif y_train.shape[0] == 0:\n",
    "    errors.append(\"y_train is empty (0 rows)\")\n",
    "\n",
    "if 'X_test' not in locals():\n",
    "    errors.append(\"X_test not defined\")\n",
    "elif X_test is None:\n",
    "    errors.append(\"X_test is None\")\n",
    "elif not hasattr(X_test, 'shape'):\n",
    "    errors.append(\"X_test has no shape attribute\")\n",
    "elif X_test.shape[0] == 0:\n",
    "    errors.append(\"X_test is empty (0 rows)\")\n",
    "\n",
    "if 'y_test' not in locals():\n",
    "    errors.append(\"y_test not defined\")\n",
    "elif y_test is None:\n",
    "    errors.append(\"y_test is None\")\n",
    "elif not hasattr(y_test, 'shape'):\n",
    "    errors.append(\"y_test has no shape attribute\")\n",
    "elif y_test.shape[0] == 0:\n",
    "    errors.append(\"y_test is empty (0 rows)\")\n",
    "\n",
    "# If any errors found, print them and exit\n",
    "if errors:\n",
    "    print(\"Validation failed with the following errors:\")\n",
    "    for error in errors:\n",
    "        print(f\"- {error}\")\n",
    "    print(\"Please fix these issues before proceeding.\")\n",
    "else:\n",
    "    print(\"All validations passed. Proceeding with model training and evaluation.\")\n",
    "    \n",
    "    # Proceed with your training and evaluation code\n",
    "    history = model.fit(X_train, y_train, epochs=10, validation_split=0.2)\n",
    "    \n",
    "    # Evaluation code\n",
    "    print(\"\\nEvaluation:\")\n",
    "    print(\"Metric names:\", model.metrics_names)\n",
    "    results = model.evaluate(X_test, y_test, verbose=0)\n",
    "    test_loss, test_acc, test_precision, test_recall = results\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Test Precision: {test_precision:.4f}\")\n",
    "    print(f\"Test Recall: {test_recall:.4f}\")\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8d5758e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__name__', '__doc__', '__package__', '__loader__', '__spec__', '__builtin__', '__builtins__', '_ih', '_oh', '_dh', 'In', 'Out', 'get_ipython', 'exit', 'quit', 'open', '_', '__', '___', '__vsc_ipynb_file__', '_i', '_ii', '_iii', '_i1', '_i2', '_exit_code', 'pd', 'np', '_i3', 'df', '_i4', 'tf', 'train_test_split', 'StandardScaler', 'LabelEncoder', 'classification_report', 'confusion_matrix', 'accuracy_score', '_i5', '_i6', '_6', '_i7', '_i8', '_i9', '_i10', 'categorical_cols', 'col', '_i11', '_i12', '_i13', '_i14', '_i15', '_i16', 'X', 'y', '_i17', '_i18', '_i19', '_i20', 'X_clean', 'y_clean', '_i21', '_i22', 'df_clean', '_i23', '_i24', '_i25', 'X_nonan', 'y_nonan', '_i26', 'valid_indices', '_i27', '_i28', '_i29', '_i30', 'create_model', '_i31', '_i32', 'Sequential', 'layers', 'metrics', '_i33', '_i34', 'early_stopping', '_i35', '_i36', '_i37', '_i38', '_i39', '_i40', '_i41', '_i42', '_i43', 'errors', 'error', '_i44', 'prepare_data', 'build_model', 'main', '_i45'])\n"
     ]
    }
   ],
   "source": [
    "def prepare_data():\n",
    "    # Your preprocessing code here\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def build_model(input_shape):\n",
    "    return create_model(input_shape)\n",
    "\n",
    "def main():\n",
    "    # 1. Prepare data\n",
    "    X_train, y_train, X_test, y_test = prepare_data()\n",
    "    \n",
    "    # 2. Build model\n",
    "    model = build_model(X_train.shape[1])\n",
    "    \n",
    "    # 3. Train and evaluate\n",
    "    history = model.fit(X_train, y_train, epochs=10, validation_split=0.2)\n",
    "    \n",
    "    # Evaluation code\n",
    "    print(\"\\nEvaluation:\")\n",
    "    results = model.evaluate(X_test, y_test, verbose=0)\n",
    "    test_loss, test_acc, test_precision, test_recall = results\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "print(locals().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "71a6fa0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Splitting data into train/test sets...\n",
      "Scaling features...\n",
      "Creating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">420,544</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │       \u001b[38;5;34m420,544\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">422,657</span> (1.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m422,657\u001b[0m (1.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">422,657</span> (1.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m422,657\u001b[0m (1.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Epoch 1/10\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.5433 - loss: 0.8386 - precision: 0.3032 - recall: 0.4816 - val_accuracy: 0.7524 - val_loss: 0.4613 - val_precision: 0.5870 - val_recall: 0.1831\n",
      "Epoch 2/10\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8412 - loss: 0.3479 - precision: 0.8349 - recall: 0.5112 - val_accuracy: 0.7755 - val_loss: 0.4588 - val_precision: 0.5875 - val_recall: 0.4780\n",
      "Epoch 3/10\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9426 - loss: 0.1671 - precision: 0.9289 - recall: 0.8534 - val_accuracy: 0.7595 - val_loss: 0.5129 - val_precision: 0.5405 - val_recall: 0.5424\n",
      "Epoch 4/10\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9790 - loss: 0.0739 - precision: 0.9635 - recall: 0.9582 - val_accuracy: 0.7533 - val_loss: 0.5831 - val_precision: 0.5272 - val_recall: 0.5593\n",
      "Epoch 5/10\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9859 - loss: 0.0415 - precision: 0.9690 - recall: 0.9768 - val_accuracy: 0.7560 - val_loss: 0.6594 - val_precision: 0.5284 - val_recall: 0.6305\n",
      "Epoch 6/10\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9890 - loss: 0.0325 - precision: 0.9796 - recall: 0.9795 - val_accuracy: 0.7533 - val_loss: 0.7155 - val_precision: 0.5227 - val_recall: 0.6644\n",
      "Epoch 7/10\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9883 - loss: 0.0300 - precision: 0.9739 - recall: 0.9819 - val_accuracy: 0.7471 - val_loss: 0.7691 - val_precision: 0.5126 - val_recall: 0.6915\n",
      "Epoch 8/10\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9853 - loss: 0.0303 - precision: 0.9754 - recall: 0.9712 - val_accuracy: 0.7391 - val_loss: 0.8355 - val_precision: 0.5012 - val_recall: 0.7288\n",
      "Epoch 9/10\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9886 - loss: 0.0259 - precision: 0.9789 - recall: 0.9790 - val_accuracy: 0.7409 - val_loss: 0.8683 - val_precision: 0.5035 - val_recall: 0.7254\n",
      "Epoch 10/10\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9888 - loss: 0.0233 - precision: 0.9766 - recall: 0.9794 - val_accuracy: 0.7294 - val_loss: 0.9591 - val_precision: 0.4890 - val_recall: 0.7559\n",
      "\n",
      "Evaluating model...\n",
      "Metric names: ['loss', 'compile_metrics']\n",
      "\n",
      "Test Accuracy: 0.7786\n",
      "Test Precision: 0.6123\n",
      "Test Recall: 0.4519\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.90      0.86      1035\n",
      "           1       0.61      0.45      0.52       374\n",
      "\n",
      "    accuracy                           0.78      1409\n",
      "   macro avg       0.72      0.67      0.69      1409\n",
      "weighted avg       0.76      0.78      0.77      1409\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[928 107]\n",
      " [205 169]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 1. Data Loading and Preprocessing\n",
    "def load_and_preprocess_data(filepath):\n",
    "    # Load data\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Preprocessing\n",
    "    df = df.drop('Customer ID', axis=1)\n",
    "    df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})\n",
    "    \n",
    "    # Handle categorical variables\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_cols:\n",
    "        if df[col].nunique() == 2:\n",
    "            df[col] = LabelEncoder().fit_transform(df[col])\n",
    "        else:\n",
    "            df = pd.get_dummies(df, columns=[col], prefix=col)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 2. Model Creation\n",
    "def create_model(input_shape):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu', input_shape=(input_shape,)),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# 3. Main Execution\n",
    "def main():\n",
    "    try:\n",
    "        # Step 1: Prepare data\n",
    "        print(\"Loading and preprocessing data...\")\n",
    "        df_processed = load_and_preprocess_data('Churn.csv')\n",
    "        X = df_processed.drop('Churn', axis=1)\n",
    "        y = df_processed['Churn']\n",
    "        \n",
    "        # Step 2: Train-test split\n",
    "        print(\"Splitting data into train/test sets...\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "        \n",
    "        # Step 3: Feature scaling\n",
    "        print(\"Scaling features...\")\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        # Step 4: Create model\n",
    "        print(\"Creating model...\")\n",
    "        input_shape = X_train.shape[1]\n",
    "        model = create_model(input_shape)\n",
    "        model.summary()\n",
    "        \n",
    "        # Step 5: Train model\n",
    "        print(\"Training model...\")\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=10,\n",
    "            batch_size=32,\n",
    "            validation_split=0.2,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Step 6: Evaluate model\n",
    "        print(\"\\nEvaluating model...\")\n",
    "        print(\"Metric names:\", model.metrics_names)\n",
    "        test_loss, test_acc, test_precision, test_recall = model.evaluate(X_test, y_test, verbose=0)\n",
    "        \n",
    "        print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n",
    "        print(f\"Test Precision: {test_precision:.4f}\")\n",
    "        print(f\"Test Recall: {test_recall:.4f}\")\n",
    "        \n",
    "        # Step 7: Generate predictions\n",
    "        y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError encountered: {str(e)}\")\n",
    "        print(\"Please check:\")\n",
    "        print(\"- Data file exists and is in correct format\")\n",
    "        print(\"- All required columns are present\")\n",
    "        print(\"- No missing values in critical fields\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b4d6d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input y contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m y = df[\u001b[33m'\u001b[39m\u001b[33mChurn\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Split into train and test sets\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m X_train, X_test, y_train, y_test = \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstratify\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Standardize numerical features\u001b[39;00m\n\u001b[32m      9\u001b[39m scaler = StandardScaler()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\model_selection\\_split.py:2940\u001b[39m, in \u001b[36mtrain_test_split\u001b[39m\u001b[34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[39m\n\u001b[32m   2936\u001b[39m         CVClass = ShuffleSplit\n\u001b[32m   2938\u001b[39m     cv = CVClass(test_size=n_test, train_size=n_train, random_state=random_state)\n\u001b[32m-> \u001b[39m\u001b[32m2940\u001b[39m     train, test = \u001b[38;5;28mnext\u001b[39m(\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstratify\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   2942\u001b[39m train, test = ensure_common_namespace_device(arrays[\u001b[32m0\u001b[39m], train, test)\n\u001b[32m   2944\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[32m   2945\u001b[39m     chain.from_iterable(\n\u001b[32m   2946\u001b[39m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[32m   2947\u001b[39m     )\n\u001b[32m   2948\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\model_selection\\_split.py:2429\u001b[39m, in \u001b[36mStratifiedShuffleSplit.split\u001b[39m\u001b[34m(self, X, y, groups)\u001b[39m\n\u001b[32m   2424\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m groups \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2425\u001b[39m     warnings.warn(\n\u001b[32m   2426\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe groups parameter is ignored by \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   2427\u001b[39m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m   2428\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2429\u001b[39m y = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43my\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2430\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().split(X, y, groups)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:1105\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1099\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1100\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound array with dim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray.ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1101\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m while dim <= 2 is required\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1102\u001b[39m     )\n\u001b[32m   1104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1107\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[32m   1113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m   1114\u001b[39m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:120\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:169\u001b[39m, in \u001b[36m_assert_all_finite_element_wise\u001b[39m\u001b[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name == \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[32m    155\u001b[39m     msg_err += (\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept missing values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#estimators-that-handle-nan-values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    168\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[31mValueError\u001b[39m: Input y contains NaN."
     ]
    }
   ],
   "source": [
    "# Split into features and target\n",
    "X = df.drop('Churn', axis=1)\n",
    "y = df['Churn']\n",
    "\n",
    "\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
